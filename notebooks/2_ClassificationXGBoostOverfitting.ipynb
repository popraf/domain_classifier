{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign = pd.read_csv('../Data/CSV_benign.csv')\n",
    "df_malware = pd.read_csv('../Data/CSV_malware.csv')\n",
    "\n",
    "# 'Country' column name is duplicated in malware csv, therefore I decided to rename both. While reading it, pandas reads duplicated column name with '.1' suffix\n",
    "df_benign.rename(columns={'Country.1':'Country_1'}, inplace=True)\n",
    "df_malware.rename(columns={'Country.1':'Country_1'}, inplace=True)\n",
    "\n",
    "# Reindex columns\n",
    "df_benign = df_benign.reindex(sorted(df_benign.columns), axis=1)\n",
    "df_malware = df_malware.reindex(sorted(df_malware.columns), axis=1)\n",
    "# df_malware.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of benign df:  (494135, 38)  | Shape of malware df:  (4999, 38)\n",
      "The dataset contains a high imbalance: among 499134 total samples (w/o dropping duplicates)\n",
      "only 4999 samples are marked as threat, which is 1.002%  (w/o dropping duplicates)\n",
      "# of columns in df_benign: 38 | # of columns in df_malware: 38\n",
      "Does two dataframes column names match? True\n",
      "Dtypes match check: \n",
      "1gram                  True\n",
      "2gram                  True\n",
      "3gram                  True\n",
      "ASN                    True\n",
      "Alexa_Rank            False\n",
      "Country                True\n",
      "Country_1              True\n",
      "Creation_Date_Time     True\n",
      "Domain                 True\n",
      "Domain_Age             True\n",
      "Domain_Name            True\n",
      "Emails                 True\n",
      "IP                     True\n",
      "Name_Server_Count     False\n",
      "Organization           True\n",
      "Page_Rank              True\n",
      "Registrant_Name        True\n",
      "Registrar              True\n",
      "State                  True\n",
      "TTL                   False\n",
      "char_distribution      True\n",
      "dec_32                False\n",
      "dec_8                 False\n",
      "entropy               False\n",
      "hex_32                False\n",
      "hex_8                 False\n",
      "len                   False\n",
      "longest_word           True\n",
      "numeric_percentage    False\n",
      "obfuscate_at_sign     False\n",
      "oc_32                 False\n",
      "oc_8                  False\n",
      "puny_coded            False\n",
      "shortened             False\n",
      "sld                    True\n",
      "subdomain             False\n",
      "tld                    True\n",
      "typos                  True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# A few basic data checks\n",
    "print('Shape of benign df: ', df_benign.shape, ' | Shape of malware df: ', df_malware.shape)\n",
    "print(f'The dataset contains a high imbalance: among {df_benign.shape[0]+df_malware.shape[0]} total samples (w/o dropping duplicates)')\n",
    "print(f'only {df_malware.shape[0]} samples are marked as threat, which is {(df_malware.shape[0]/(df_benign.shape[0]+df_malware.shape[0]))*100:.3f}%  (w/o dropping duplicates)')\n",
    "print(f'# of columns in df_benign: {df_benign.columns.value_counts().sum()} | # of columns in df_malware: {df_malware.columns.value_counts().sum()}')\n",
    "print(f'Does two dataframes column names match? {df_benign.columns.tolist().sort() == df_malware.columns.tolist().sort()}')\n",
    "print(f'Dtypes match check: \\n{df_benign.dtypes == df_malware.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By digging into missmatch in dtypes, I was able to identify 'mixing' of columns data in 24 records of df_malware.\n",
    "# To fix it, following steps are taken: 1. Identify incorrect rows by checking len of IP column,\n",
    "#   2. Get records into new df\n",
    "#   3. Rename columns\n",
    "#   4. Drop incorrect rows from df_malware\n",
    "#   5. Concatenate fixed data to df_malware\n",
    "\n",
    "incorrect_rows_idx = df_malware.index[df_malware['IP'].str.len()==2]\n",
    "df_incorrect_rows = df_malware.iloc[incorrect_rows_idx]\n",
    "\n",
    "# Applies to df_malware only - rename of columns for 24 records\n",
    "col_val_replace_to = {\n",
    "    'Country': 'TTL',\n",
    "    'TTL': 'Domain',\n",
    "    'IP': 'Country',\n",
    "    'Domain': 'IP',\n",
    "}\n",
    "\n",
    "df_incorrect_rows.rename(columns=col_val_replace_to, inplace=True) # Apply rename\n",
    "df_malware.drop(incorrect_rows_idx, axis=0, inplace=True) # Drop from malware df incorrect rows\n",
    "df_malware = pd.concat([df_malware, df_incorrect_rows], ignore_index=False) # Concatenate fixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_benign.index[df_benign['IP'].str.len()==2] # Proof: this case does not affect benign df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop from malware df 2 records with domain equal to 397220 (incorrectly imputed data)\n",
    "df_malware.drop(df_malware.index[df_malware['Domain']=='397220'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to make sure, domains in benign df are also checked (by looking at csv) - after checking it domains look fine\n",
    "unique_domains_benign = df_benign['Domain'].unique()\n",
    "# pd.DataFrame(unique_domains_benign).to_csv('./tmp/unique_domains_benign.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df shape before the filling process & dropping duplicates:  (4997, 38)\n",
      "After filling & drop of duplicates:  (4205, 37)\n",
      "Checking for nulls not filled: \n",
      " 1gram                    0\n",
      "2gram                    0\n",
      "3gram                    0\n",
      "ASN                     55\n",
      "Alexa_Rank             457\n",
      "Country                 57\n",
      "Country_1             1780\n",
      "Creation_Date_Time     950\n",
      "Domain                   0\n",
      "Domain_Name            823\n",
      "Emails                1342\n",
      "IP                       9\n",
      "Name_Server_Count      457\n",
      "Organization          2310\n",
      "Page_Rank              457\n",
      "Registrant_Name       4139\n",
      "Registrar             1066\n",
      "State                 2010\n",
      "TTL                      0\n",
      "char_distribution        0\n",
      "dec_32                   0\n",
      "dec_8                    0\n",
      "entropy                  0\n",
      "hex_32                   0\n",
      "hex_8                    0\n",
      "len                      0\n",
      "longest_word             0\n",
      "numeric_percentage       0\n",
      "obfuscate_at_sign        0\n",
      "oc_32                    0\n",
      "oc_8                     0\n",
      "puny_coded               0\n",
      "shortened               51\n",
      "sld                      0\n",
      "subdomain                0\n",
      "tld                      0\n",
      "typos                    0\n",
      "dtype: int64\n",
      "Df shape before the filling process & dropping duplicates:  (494135, 38)\n",
      "After filling & drop of duplicates:  (370170, 37)\n",
      "Checking for nulls not filled: \n",
      " 1gram                     22\n",
      "2gram                    142\n",
      "3gram                      0\n",
      "ASN                     1168\n",
      "Alexa_Rank             32355\n",
      "Country                 1226\n",
      "Country_1             170080\n",
      "Creation_Date_Time     89011\n",
      "Domain                     0\n",
      "Domain_Name            62099\n",
      "Emails                118224\n",
      "IP                       166\n",
      "Name_Server_Count      32389\n",
      "Organization          210760\n",
      "Page_Rank              32781\n",
      "Registrant_Name       359379\n",
      "Registrar              95168\n",
      "State                 197763\n",
      "TTL                        0\n",
      "char_distribution          0\n",
      "dec_32                    90\n",
      "dec_8                     10\n",
      "entropy                    3\n",
      "hex_32                    77\n",
      "hex_8                      0\n",
      "len                      298\n",
      "longest_word             103\n",
      "numeric_percentage       734\n",
      "obfuscate_at_sign          0\n",
      "oc_32                      0\n",
      "oc_8                       0\n",
      "puny_coded                 0\n",
      "shortened               1033\n",
      "sld                      978\n",
      "subdomain                 15\n",
      "tld                        3\n",
      "typos                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dataset contains a lot of null/nan in columns like e.g. Country, ASN, IP, Domain_Name, State and so on.\n",
    "# It is a common knowledge that ASN and IP might change (e.g. by a load balancer), however acording to\n",
    "# Mahdavifar et al. (2021) the data has been gathered in a sliding window τ, which was relatively short (approx. 0,7s for b'instagram.com.' domain in benign df).\n",
    "# Therefore, I made an assumption that only [Domain_Age] might be different across samples of the same domain.\n",
    "# In the code below I used forward and backward filling of missing values for certain domain, \n",
    "#   followed by work on nulls and drop of duplicates.\n",
    "# Primary task is to classify malicious domains, therefore drop of redundant data is reasonable (note that it might be distincted by [Domain_Age]).\n",
    "\n",
    "def custom_filling(df):\n",
    "    if not df['Domain'].isnull().sum().sum() == 0:\n",
    "        raise Exception(\"Exception: The dataset contains null values in Domain column! Please provide correct dataset or change filling approach.\")\n",
    "    \n",
    "    print('Df shape before the filling process & dropping duplicates: ', df.shape)\n",
    "    # df = df.groupby(['Domain']).apply(lambda group: group.ffill()) # TODO/Done: Improve performance + bfill\n",
    "\n",
    "    df['Domain_tmp'] = df['Domain'] # The reason to do it this way is that as_index is ignored in groupby op, and it is anyway faster than apply approach\n",
    "    df = df.groupby(['Domain_tmp'], as_index=False).ffill()\n",
    "\n",
    "    df['Domain_tmp'] = df['Domain'] # The reason to do it this way is that as_index is ignored in groupby op, and it is anyway faster than apply approach\n",
    "    df = df.groupby(['Domain_tmp'], as_index=False).bfill()\n",
    "    df.drop(columns=['Domain_Age'], inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print('After filling & drop of duplicates: ', df.shape)\n",
    "    print('Checking for nulls not filled: \\n', df.isnull().sum(axis = 0))\n",
    "    return df\n",
    "\n",
    "df_malware = custom_filling(df_malware)\n",
    "df_benign = custom_filling(df_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mitigate missing values across similar columns like Domain, Domain_Name and Country, Country_1, following code is applied to df's\n",
    "# The code also applies mapping to unify a bit entries\n",
    "countries_map = {\n",
    "    '-':'',\n",
    "    \"china\":\"CN\",\n",
    "    \"Malaysia\":'ID',\n",
    "    \"United States\":\"US\",\n",
    "    \"TURKEY\":'TR',\n",
    "    'RUSSIA':'RU',\n",
    "    'Russian Federation':'RU',\n",
    "    'Belarus':'BY',\n",
    "    'Korea':'KR',\n",
    "}\n",
    "\n",
    "def use_regex(input_text):\n",
    "    return re.sub(r\"b'(.+?).'\", r\"\\1\", input_text)\n",
    "\n",
    "def impute_similar_cols(df):\n",
    "    df[\"Country_1\"].replace(countries_map, inplace=True)\n",
    "    df[\"Country\"].replace(countries_map, inplace=True)\n",
    "    df[\"Country_1\"].fillna(df[\"Country\"], inplace=True)\n",
    "    df[\"Country\"].fillna(df[\"Country_1\"], inplace=True)\n",
    "    df[\"Domain_Name\"].fillna(df[\"Domain\"].apply(use_regex), inplace=True)\n",
    "    return df\n",
    "\n",
    "df_malware = impute_similar_cols(df_malware)\n",
    "df_benign = impute_similar_cols(df_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtypes match check: \n",
      "1gram                  True\n",
      "2gram                  True\n",
      "3gram                  True\n",
      "ASN                    True\n",
      "Alexa_Rank            False\n",
      "Country                True\n",
      "Country_1              True\n",
      "Creation_Date_Time     True\n",
      "Domain                 True\n",
      "Domain_Name            True\n",
      "Emails                 True\n",
      "IP                     True\n",
      "Name_Server_Count     False\n",
      "Organization           True\n",
      "Page_Rank              True\n",
      "Registrant_Name        True\n",
      "Registrar              True\n",
      "State                  True\n",
      "TTL                   False\n",
      "char_distribution      True\n",
      "dec_32                False\n",
      "dec_8                 False\n",
      "entropy               False\n",
      "hex_32                False\n",
      "hex_8                 False\n",
      "len                   False\n",
      "longest_word           True\n",
      "numeric_percentage    False\n",
      "obfuscate_at_sign     False\n",
      "oc_32                 False\n",
      "oc_8                  False\n",
      "puny_coded            False\n",
      "shortened             False\n",
      "sld                    True\n",
      "subdomain             False\n",
      "tld                    True\n",
      "typos                  True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(f'Dtypes match check: \\n{df_benign.dtypes == df_malware.dtypes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all columns with missmatch data type are being processed in this function\n",
    "def process_cols_to_type(df):\n",
    "    df['Alexa_Rank'] = pd.to_numeric(df[\"Alexa_Rank\"], errors='coerce') # Should be number\n",
    "    df['Alexa_Rank'].fillna(0, inplace=True)\n",
    "    df['Name_Server_Count'] = pd.to_numeric(df[\"Name_Server_Count\"], errors='coerce') # Should be number\n",
    "    df['TTL'] = pd.to_numeric(df[\"TTL\"], errors='coerce') # Should be number\n",
    "    df['entropy'] = pd.to_numeric(df[\"entropy\"], errors='coerce') # Should be number\n",
    "    df['len'] = pd.to_numeric(df[\"len\"], errors='coerce') # Should be number - Length of domain and subdomain\n",
    "    df['len'].fillna(len(df['Domain_Name']), inplace=True)\n",
    "    df['numeric_percentage'] = pd.to_numeric(df[\"numeric_percentage\"], errors='coerce') # Should be number - Counts the number of digits in domain and subdomain\n",
    "    df['Registrant_Name'] = df['Registrant_Name'].astype('category')\n",
    "    df['Organization'] = df['Organization'].astype('category')\n",
    "    df['Country'] = df['Country'].astype('category')\n",
    "    df['Emails'] = df['Emails'].astype('category')\n",
    "    df['State'] = df['State'].astype('category')\n",
    "    df['tld'] = df['tld'].astype('category')\n",
    "    df['Registrar'] = df['Registrar'].astype('category')\n",
    "    df['Domain_Name'] = df['Domain_Name'].astype('category')\n",
    "    df['sld'] = df['sld'].astype('category')\n",
    "    return df\n",
    "\n",
    "# selected_cols = ['Registrant_Name','Organization','Country','Emails','State','tld','Registrar','Domain_Name','TTL','sld','entropy']\n",
    "df_malware = process_cols_to_type(df_malware)\n",
    "df_benign = process_cols_to_type(df_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store data to csv for manual analysis\n",
    "# df_benign.to_csv('./tmp/df_benign_filled.csv')\n",
    "# df_malware.to_csv('./tmp/df_malware_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign = process_cols_to_type(df_benign)\n",
    "df_malware = process_cols_to_type(df_malware)\n",
    "\n",
    "# Select relevant columns\n",
    "selected_cols = ['Alexa_Rank','Registrant_Name','Organization','Country','Emails','State','tld','Registrar','Domain_Name','TTL','sld','entropy']\n",
    "\n",
    "df_benign = df_benign[selected_cols]\n",
    "df_malware = df_malware[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign['is_threat'] = 0\n",
    "df_malware['is_threat'] = 1\n",
    "\n",
    "valid_benign = df_benign.iloc[-100:].copy()\n",
    "df_benign = df_benign.iloc[:-100]\n",
    "valid_malware = df_malware.iloc[-100:].copy()\n",
    "df_malware = df_malware.iloc[:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two dataframes\n",
    "df = pd.concat([df_benign, df_malware])\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = df.drop(columns='is_threat')\n",
    "y = df['is_threat']\n",
    "\n",
    "df_valid = pd.concat([valid_benign, valid_malware])\n",
    "df_valid.drop_duplicates(inplace=True)\n",
    "df_valid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_valid = df_valid.drop(columns='is_threat')\n",
    "y_valid = df_valid['is_threat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "def balance_data(X_part, y_part):\n",
    "    X_bal, y_bal = SMOTETomek(sampling_strategy='auto').fit_resample(X_part, y_part)\n",
    "    return X_bal, y_bal\n",
    "\n",
    "# X, y = balance_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.99\n",
      "Average Balanced Accuracy: 0.52\n",
      "Average F1 Score: 0.98\n",
      "Average ROC AUC: 0.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "scaler = MinMaxScaler()\n",
    "clf = XGBClassifier()\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize variables to store evaluation metrics\n",
    "accuracy_scores = []\n",
    "balanced_acc_scores = []\n",
    "f1_scores = []\n",
    "roc_aucs = []\n",
    "\n",
    "# Train and evaluate the classifier for each split\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    for col in X_train.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(X_train[col]):\n",
    "                X_train.loc[:, col] = label_encoder.fit_transform(X_train[col])\n",
    "\n",
    "    for col in X_test.columns:\n",
    "            if not pd.api.types.is_numeric_dtype(X_test[col]):\n",
    "                X_test.loc[:, col] = label_encoder.fit_transform(X_test[col])\n",
    "\n",
    "    scaler.fit(X_train.fillna(0))  # Fit on the training data\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train.fillna(0)), columns=X_train.columns)  # Transform the training data\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test.fillna(0)), columns=X_test.columns)    # Transform the test data\n",
    "\n",
    "    # X_train, y_train = balance_data(X_train, y_train)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    balanced_acc_scores.append(balanced_acc)\n",
    "    f1_scores.append(f1)\n",
    "    roc_aucs.append(roc_auc)\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_balanced_acc = np.mean(balanced_acc_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_roc_auc = np.mean(roc_aucs)\n",
    "\n",
    "print(f\"Average Accuracy: {avg_accuracy:.2f}\")\n",
    "print(f\"Average Balanced Accuracy: {avg_balanced_acc:.2f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.2f}\")\n",
    "print(f\"Average ROC AUC: {avg_roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.49\n",
      "Validation Balanced Accuracy: 0.50\n",
      "Validation F1 Score: 0.32\n",
      "Validation ROC AUC: 0.65\n"
     ]
    }
   ],
   "source": [
    "for col in X_valid.columns:\n",
    "    if not pd.api.types.is_numeric_dtype(X_valid[col]):\n",
    "        X_valid.loc[:, col] = label_encoder.fit_transform(X_valid[col])\n",
    "\n",
    "X_valid = pd.DataFrame(scaler.transform(X_valid.fillna(0)), columns=X_valid.columns)\n",
    "y_val_pred = clf.predict(X_valid)\n",
    "\n",
    "val_accuracy = accuracy_score(y_valid, y_val_pred)\n",
    "val_balanced_acc = balanced_accuracy_score(y_valid, y_val_pred)\n",
    "val_f1 = f1_score(y_valid, y_val_pred, average='weighted')\n",
    "y_val_prob = clf.predict_proba(X_valid)[:, 1]\n",
    "val_roc_auc = roc_auc_score(y_valid, y_val_prob)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Validation Balanced Accuracy: {val_balanced_acc:.2f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.2f}\")\n",
    "print(f\"Validation ROC AUC: {val_roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_threat\n",
       "0    362878\n",
       "1      4056\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n",
      "Number of 1s in y_test: 811  |  Number of 1s in y_train: 3245\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    num_ones_in_y_test = (y_test == 1).sum()\n",
    "    num_ones_in_y_train = (y_train == 1).sum()\n",
    "    print('Number of 1s in y_test:', num_ones_in_y_test,' | ', 'Number of 1s in y_train:', num_ones_in_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_threat\n",
       "0    362878\n",
       "1      4056\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Samaneh Mahdavifar, Nasim Maleki, Arash Habibi Lashkari, Matt Broda, Amir H. Razavi, “Classifying Malicious Domains using DNS Traffic Analysis”, The 19th IEEE International Conference on Dependable, Autonomic, and Secure Computing (DASC), Oct. 25-28, 2021, Calgary, Canada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domain_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
